<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content=" ">
  <meta name="description" content=" ">
  <meta name="keywords" content=" ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Welcome to My Homepage</title>
 
 
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <style>
    .centered {
      display: flex;
      justify-content: center;
      align-items: center;
      /* height: 100vh; 让内容垂直居中 */
      /* background-color: #f0f0f0; 添加一个背景色以便更好地看到居中效果 */
    }
  </style>

  <style>
    .shadowed-text {
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); /* 水平偏移量，垂直偏移量，模糊半径和阴影颜色 */
    }
  </style>

  <style>
    .popup {
      display: none; /* 初始状态下隐藏弹窗 */
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      width: 750px;
      height: 200px;
      background-color: #fff;
      border: 1px solid #ccc;
      padding: 20px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);
    }
    </style>

<style>
  .arrow {
    cursor: pointer;
  }
  .content {
    display: none;
  }
</style>

</head>

<body>
<div class="centered">
  <script src="../typewriter.js"></script>
  <div style='background-color: #000;height: 0px; float:center;font-family:"Consolas";'><br>
      <strong style='color:#bd4527;font-family:"Segoe UI";'>thomas</strong><strong style='color:#000000;font-family:"Segoe UI";'>@</strong><strong style='color:#66c428;font-family:"Segoe UI";'>wang</strong><a style='color:#58ACFA;font-family:"Segoe UI";'>:~/$ </a>
      <a id="btapp" style='color:#000;font-family:"Segoe UI";'></a>
      <script>
          const btapp = document.getElementById('btapp');
          const bttypewriter = new Typewriter(btapp, {
              loop: true
          });
          bttypewriter.typeString("The only true wisdom is in knowing you know anything.")
              .pauseFor(800)
              .deleteChars(9)
              .typeString('nothing.')
              .pauseFor(800)
              .deleteAll()
              .typeString('The beautiful thing about learning is that no one can take it away from you.')
              .pauseFor(800)
              .deleteAll()
              .start();
      </script>
  </div>
</div>
<br>
<br>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yubin Wang&nbsp; &nbsp;(王雨滨)</name><br>
                <h5 style="text-align:center">Email: yubinwang628@gmail.com&nbsp; &nbsp;|&nbsp; &nbsp;wangyubin2018@tongji.edu.cn</h6>
              </p>
              <div class="w3-content" style="text-align: justify">
              I am a second-year master's student at Tongji University, luckily advised by <a href='https://vill-lab.github.io/'>Prof. Cairong Zhao</a>. 
              Prior to this, I received my bachelor degree in Data Science and Big Data from Tongji University in 2022. 
              At present, I am doing research intern in AI/ML Group of Microsoft Research Asia, Shanghai, supervised by 
              <a href='https://scholar.google.com/citations?user=JiTfWVMAAAAJ'>Dr. Xinyang Jiang</a> and <a href='http://recmind.cn/'>Dr. Dongsheng Li</a>. 
              Before this, I was working at Baidu Inc. as a research intern, closely with <a href='https://bigteacher-777.github.io/'>Zhikang Zou</a> 
              and <a href='https://shuluoshu.github.io/'>Dr. Xiaoqing Ye</a>. 
              I also have a deep academic collaboration with <a href='https://web.xidian.edu.cn/dcheng/index.html'>Prof. De Cheng</a> from Xidian University. 
              My research interests are in computer vision and multi-modal learning, with specific interest in <b>prompt learning</b>, 
              explainibility and knowledge discovery in vision, text-based person re-id, video temporal grounding, point-based 3D detection, etc.
              </p>
              </div>
              <p style="text-align:center">
                <a class="shadowed-text" style='color:#3aba69' href="https://scholar.google.com/citations?user=mLeYNLoAAAAJ">Google Scholar</a>&nbsp; &nbsp;&nbsp; &nbsp;<a class="shadowed-text" style='color:#977937' href="https://github.com/ThomasWangY">Github</a>
              </p>
            </td>
            <td style="padding:5% 3% 3% 3%;width:40%;max-width:40%">
              <a><img style="width:100%;max-width:120%" alt="profile photo" src="profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
      </tbody></table>


      <!-- ------------------------ News------------------------------- -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <hr />
            <p>
              2024.07: Joined MSRA Shanghai as a research intern. Focus on explainibility and knowledge discovery in vision.<br>
              2024.02: One paper about prompt learning accepted by TIP.<br>
              2024.01: Joined Baidu Inc. at Shanghai as a research intern. Focus on 3D vision.<br>
              2023.12: One paper about prompt learning accepted by AAAI 2024.<br>
              2022.09: Became a graduate student at Tongji University.<br>
              2022.07: One paper about text-based person re-id accepted by PRCV 2022, Oral.<br>
              2021.07: Joined VILL Lab, advised by Prof. Cairong Zhao.<br>
              2021.05: My first paper about opinion summarization accepted by IJCRS 2021.<br>
            </p>
          </td>
        </tr>
      </tbody></table>

      <!-- ------------------------ publications------------------------------- -->

      <table width="100%" align="center" border="0" cellspacing="0" style="padding-top:20px;padding-left:20px;padding-bottom:5px"><tbody>
        <tr> <td>
                <heading>Publications</heading>  (<strong>*</strong>equal contribution; only papers as first authors are included; double click to view abstract)
                <hr />
          </td> </tr>
      </tbody> </table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr></tr>
      <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
        <div class="one" >
            <img src='./Publications/img/metaprompt.png' style="height:100%;width:160%; position: absolute;top: -0%">
        </div>
      </td>

      <td style="padding:40px;width:75%;vertical-align:middle">
          <papertitle>Learning Domain Invariant Prompt for Vision-Language Models</papertitle>
          <br>
          Cairong Zhao<strong>*</strong>, <strong>Yubin Wang*</strong>, Xinyang Jiang, Yifei Shen, Kaitao Song, Dongsheng Li, Duoqian Miao
          <br>
          <em>IEEE Transactions on Image Processing</em>, TIP (CCF-A, SCI)
          <br>
          <a href="./Publications/pdf/metaprompt.pdf">[PDF]</a>
          <a href="https://github.com/Vill-Lab/2024-TIP-MetaPrompt">[Code]</a>
          <a href="#" onclick="showPopup()">[BibTeX]</a>
          <br>
          <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
          <div class="content">
            <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">Prompt learning stands out as one of the most efficient approaches for adapting powerful vision-language foundational models like CLIP to downstream datasets by tuning learnable prompt vectors with very few samples. However, despite its
              success in achieving remarkable performance on in-domain data,
              prompt learning still faces the significant challenge of effectively
              generalizing to novel classes and domains. Some existing methods
              address this concern by dynamically generating distinct prompts
              for different domains. Yet, they overlook the inherent potential of
              prompts to generalize across unseen domains. To address these
              limitations, our study introduces an innovative prompt learning
              paradigm, called MetaPrompt, aiming to directly learn domain
              invariant prompt in few-shot scenarios. To facilitate learning
              prompts for image and text inputs independently, we present
              a dual-modality prompt tuning network comprising two pairs
              of coupled encoders. Our study centers on an alternate episodic
              training algorithm to enrich the generalization capacity of the
              learned prompts. In contrast to traditional episodic training
              algorithms, our approach incorporates both in-domain updates
              and domain-split updates in a batch-wise manner. For in-domain
              updates, we introduce a novel asymmetric contrastive learning
              paradigm, where representations from the pre-trained encoder
              assume supervision to regularize prompts from the prompted
              encoder. To enhance performance on out-of-domain distribution,
              we propose a domain-split optimization on visual prompts for
              cross-domain tasks or textual prompts for cross-class tasks
              during domain-split updates. Extensive experiments across 11
              datasets for base-to-new generalization and 4 datasets for domain
              generalization exhibit favorable performance. Compared with
              the state-of-the-art method, MetaPrompt achieves an absolute
              gain of 1.02% on the overall harmonic mean in base-to-new
              generalization and consistently demonstrates superiority over all
              benchmarks in domain generalization.</p>
            </h6>

          <script>
          function toggleContent(element) {
            var content = element.nextElementSibling;
            if (content.style.display === "none") {
              content.style.display = "block";
              element.textContent = "▼ Abstract";
            } else {
              content.style.display = "none";
              element.textContent = "▶ Abstract";
            }
          }
          </script>
          <div class="w3-content" style="text-align: justify">
          <p>
          </p>
          </div>
      </td>
      <div id="popup" class="popup">
        <!-- 这里是弹窗中的自定义内容 -->
        <h6 style="font-weight:lighter;">@article{zhao2024learning, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Learning Domain Invariant Prompt for Vision-Language Models}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Zhao, Cairong and Wang, Yubin and Jiang, Xinyang and Shen, Yifei and Song, Kaitao and Li, Dongsheng and Miao, Duoqian}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;journal={IEEE Transactions on Image Processing}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;volume={33}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;pages={1348--1360}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2024}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;publisher={IEEE} <br>
        }</h6>

        <button onclick="hidePopup()">close</button>
      </div>
      
      <script>
      function showPopup() {
        document.getElementById("popup").style.display = "block"; // 显示弹窗
      }
      
      function hidePopup() {
        document.getElementById("popup").style.display = "none"; // 隐藏弹窗
      }
      </script>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr></tr>
      <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
        <div class="one" >
            <img src='./Publications/img/hpt.png' style="height:100%;width:160%; position: absolute;top: -0%">
        </div>
      </td>

      <td style="padding:40px;width:75%;vertical-align:middle">
          <papertitle>Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models</papertitle>
          <br>
          <strong>Yubin Wang</strong>, Xinyang Jiang, De Cheng, Dongsheng Li, Cairong Zhao
          <br>
          <em>The 38th Annual AAAI Conference on Artificial Intelligence</em>, AAAI 2024 (CCF-A)
          <br>
          <a href="./Publications/pdf/hpt.pdf">[PDF]</a>
          <a href="https://github.com/Vill-Lab/2024-AAAI-HPT">[Code]</a>
          <a href="#" onclick="showPopup1()">[BibTeX]</a>
          <br>
          <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
          <div class="content">
            <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">Prompt learning has become a prevalent strategy for adapting
              vision-language foundation models to downstream tasks. As
              large language models (LLMs) have emerged, recent studies
              have explored the use of category-related descriptions as input to enhance prompt effectiveness. Nevertheless, conventional descriptions fall short of structured information that
              effectively represents the interconnections among entities or
              attributes linked to a particular category. To address this limitation and prioritize harnessing structured knowledge, this
              paper advocates for leveraging LLMs to build a graph for
              each description to model the entities and attributes describing the category, as well as their correlations. Preexisting
              prompt tuning methods exhibit inadequacies in managing
              this structured knowledge. Consequently, we propose a novel
              approach called Hierarchical Prompt Tuning (HPT), which
              enables simultaneous modeling of both structured and conventional linguistic knowledge. Specifically, we introduce a
              relationship-guided attention module to capture pair-wise associations among entities and attributes for low-level prompt
              learning. In addition, by incorporating high-level and globallevel prompts modeling overall semantics, the proposed hierarchical structure forges cross-level interlinks and empowers the model to handle more complex and long-term relationships. Extensive experiments demonstrate that our HPT
              shows strong effectiveness and generalizes much better than
              existing SOTA methods.</p>
            </h6>

          <script>
          function toggleContent(element) {
            var content = element.nextElementSibling;
            if (content.style.display === "none") {
              content.style.display = "block";
              element.textContent = "▼ Abstract";
            } else {
              content.style.display = "none";
              element.textContent = "▶ Abstract";
            }
          }
          </script>
          <div class="w3-content" style="text-align: justify">
          <p>
          </p>
          </div>
      </td>
      <div id="popup1" class="popup">
        <!-- 这里是弹窗中的自定义内容 -->
        <h6 style="font-weight:lighter;">@inproceedings{wang2024learning, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Yubin and Jiang, Xinyang and Cheng, De and Li, Dongsheng and Zhao, Cairong}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;volume={38}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;number={6}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;pages={5749--5757}, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2024} <br>
        }</h6>
        <button onclick="hidePopup1()">close</button>
      </div>

      <script>
      function showPopup1() {
        document.getElementById("popup1").style.display = "block"; // 显示弹窗
      }

      function hidePopup1() {
        document.getElementById("popup1").style.display = "none"; // 隐藏弹窗
      }
      </script>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
  <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
    <div class="one" >
        <img src='./Publications/img/pman.png' style="height:100%;width:160%; position: absolute;top: -0%">
    </div>
  </td>

  <td style="padding:40px;width:75%;vertical-align:middle">
      <papertitle>Part-Based Multi-Scale Attention Network for Text-Based Person Search</papertitle>
      <br>
      <strong>Yubin Wang</strong>, Ding Qi, Cairong Zhao
      <br>
      <em>Chinese Conference on Pattern Recognition and Computer Vision</em>, PRCV 2022 (CCF-C, Oral)
      <br>
      <a href="./Publications/pdf/pman.pdf">[PDF]</a>
      <a href="#" onclick="showPopup2()">[BibTeX]</a>
      <br>
      <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
      <div class="content">
        <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">Text-based person search aims to retrieve the target person in an image gallery based on textual descriptions. Solving such a
          fine-grained cross-modal retrieval problem is very challenging due
          to differences between modalities. Moreover, the inter-class variance of both
          person images and descriptions is small, and more semantic information
          is needed to assist in aligning visual and textual representations
          at different scales. In this paper, we propose a Part-based Multi-Scale Attention
          Network (PMAN) capable of extracting visual semantic features from
          different scales and matching them with textual features. We initially
          extract visual and textual features using ResNet and BERT, respectively.
          Multi-scale visual semantics is then acquired based on local feature maps
          of different scales. Our proposed method learns representations for both
          modalities simultaneously based mainly on Bottleneck Transformer with
          self-attention mechanism. A multi-scale cross-modal matching strategy
          is introduced to narrow the gap between modalities from multiple scales.
          Extensive experimental results show that our method outperforms the
          state-of-the-art methods on CUHK-PEDES datasets.</p>
        </h6>

      <script>
      function toggleContent(element) {
        var content = element.nextElementSibling;
        if (content.style.display === "none") {
          content.style.display = "block";
          element.textContent = "▼ Abstract";
        } else {
          content.style.display = "none";
          element.textContent = "▶ Abstract";
        }
      }
      </script>
      <div class="w3-content" style="text-align: justify">
      <p>
      </p>
      </div>
  </td>
  <div id="popup2" class="popup">
    <!-- 这里是弹窗中的自定义内容 -->
    <h6 style="font-weight:lighter;">@inproceedings{wang2022part, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Part-Based Multi-Scale Attention Network for Text-Based Person Search}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Yubin and Qi, Ding and Zhao, Cairong}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Chinese Conference on Pattern Recognition and Computer Vision (PRCV)}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;pages={462--474}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2022}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;organization={Springer} <br>
    }</h6>

    <button onclick="hidePopup2()">close</button>
  </div>

  <script>
  function showPopup2() {
    document.getElementById("popup2").style.display = "block"; // 显示弹窗
  }

  function hidePopup2() {
    document.getElementById("popup2").style.display = "none"; // 隐藏弹窗
  }
  </script>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
  <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
    <div class="one" >
        <img src='./Publications/img/opsum.png' style="height:100%;width:160%; position: absolute;top: -0%">
    </div>
  </td>

  <td style="padding:40px;width:75%;vertical-align:middle">
      <papertitle>An Opinion Summarization-Evaluation System Based on Pre-trained Models</papertitle>
      <br>
      Han Jiang<strong>*</strong>, <strong>Yubin Wang*</strong>, Songhao Lv, Zhihua Wei
      <br>
      <em>Rough Sets: International Joint Conference</em>, IJCRS 2021
      <br>
      <a href="./Publications/pdf/opsum.pdf">[PDF]</a>
      <a href="#" onclick="showPopup3()">[BibTeX]</a>
      <br>
      <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
      <div class="content">
        <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">As social media appeal more frequently used, the task of extracting
          the mainstream opinions of the discussions arising from the media, i. e. opinion
          summarization, has drawn considerable attention. This paper proposes an opinion
          summarization-evaluation system containing a pipeline and an evaluation module
          for the task. In our algorithm, the state-of-the-art pre-trained model BERT is
          fine-tuned for the subjectivity analysis, and the advanced pre-trained models are
          combined with traditional data mining algorithms to gain the mainstreams. For
          evaluation, a set of hierarchical metrics is also stated. Experiment result shows
          that our algorithm produces concise and major opinions. An ablation study is also
          conducted to prove that each part of the pipeline takes effect significantly.</p>
        </h6>

      <script>
      function toggleContent(element) {
        var content = element.nextElementSibling;
        if (content.style.display === "none") {
          content.style.display = "block";
          element.textContent = "▼ Abstract";
        } else {
          content.style.display = "none";
          element.textContent = "▶ Abstract";
        }
      }
      </script>
      <div class="w3-content" style="text-align: justify">
      <p>
      </p>
      </div>
  </td>
  <div id="popup3" class="popup">
    <!-- 这里是弹窗中的自定义内容 -->
    <h6 style="font-weight:lighter;">@inproceedings{jiang2021opinion, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={An Opinion Summarization-Evaluation System Based on Pre-trained Models}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Jiang, Han and Wang, Yubin and Lv, Songhao and Wei, Zhihua}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Rough Sets: International Joint Conference, IJCRS 2021, Bratislava, Slovakia, September 19--24, 2021, Proceedings}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;pages={225--230}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2021}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;organization={Springer} <br>
    }</h6>

    <button onclick="hidePopup3()">close</button>
  </div>

  <script>
  function showPopup3() {
    document.getElementById("popup3").style.display = "block"; // 显示弹窗
  }

  function hidePopup3() {
    document.getElementById("popup3").style.display = "none"; // 隐藏弹窗
  }
  </script>
      
<table width="100%" align="center" border="0" cellspacing="0" style="padding-top:20px;padding-left:20px;padding-bottom:5px"><tbody>
  <tr> <td>
          <heading>Preprint or Unpublished Papers</heading>
          <hr />
    </td> </tr>
</tbody> </table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
  <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
    <div class="one" >
        <img src='./Publications/img/actprompt.png' style="height:100%;width:160%; position: absolute;top: -0%">
    </div>
  </td>

  <td style="padding:40px;width:75%;vertical-align:middle">
      <papertitle>ActPrompt: In-Domain Feature Adaptation via Action Cues for Video Temporal Grounding</papertitle>
      <br>
      <strong>Yubin Wang</strong>, Xinyang Jiang, De Cheng, Dongsheng Li, Cairong Zhao
      <br>
      <em>arXiv preprint arXiv:2408.06622, 2024.</em> Under AAAI 2025 peer review
      <br>
      <a href="./Publications/pdf/actprompt.pdf">[PDF]</a>
      <a href="#" onclick="showPopup4()">[BibTeX]</a>
      <br>
      <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
      <div class="content">
        <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">Video temporal grounding is an emerging topic aiming to
          identify specific clips within videos. In addition to pre-trained 
          video models, contemporary methods utilize pre-trained
          vision-language models (VLM) to capture detailed
          characteristics of diverse scenes and objects from video
          frames. However, as pre-trained on images, VLM may struggle
          to distinguish action-sensitive patterns from static objects,
          making it necessary to adapt them to specific data
          domains for effective feature representation over temporal
          grounding. We address two primary challenges to achieve this
          goal. Specifically, to mitigate high adaptation costs, we propose
           an efficient preliminary in-domain fine-tuning paradigm
          for feature adaptation, where downstream-adaptive features
          are learned through several pretext tasks. Furthermore, to
          integrate action-sensitive information into VLM, we introduce
          Action-Cue-Injected Temporal Prompt Learning (ActPrompt), 
          which injects action cues into the image encoder of
          VLM for better discovering action-sensitive patterns. Extensive
          experiments demonstrate that ActPrompt is an off-the-shelf 
          training framework that can be effectively applied to
          various SOTA methods, resulting in notable improvements.
        </p>
        </h6>

      <script>
      function toggleContent(element) {
        var content = element.nextElementSibling;
        if (content.style.display === "none") {
          content.style.display = "block";
          element.textContent = "▼ Abstract";
        } else {
          content.style.display = "none";
          element.textContent = "▶ Abstract";
        }
      }
      </script>
      <div class="w3-content" style="text-align: justify">
      <p>
      </p>
      </div>
  </td>
  <div id="popup4" class="popup">
    <!-- 这里是弹窗中的自定义内容 -->
    <h6 style="font-weight:lighter;">@article{wang2024actprompt, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={ActPrompt: In-Domain Feature Adaptation via Action Cues for Video Temporal Grounding}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Yubin and Jiang, Xinyang and Cheng, De and Li, Dongsheng and Zhao, Cairong}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2408.06622}, <br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2024} <br>
    }</h6>

    <button onclick="hidePopup4()">close</button>
  </div>

  <script>
  function showPopup4() {
    document.getElementById("popup4").style.display = "block"; // 显示弹窗
  }

  function hidePopup4() {
    document.getElementById("popup4").style.display = "none"; // 隐藏弹窗
  }
  </script>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
  <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
    <div class="one" >
        <img src='./Publications/img/uni2det.png' style="height:100%;width:160%; position: absolute;top: -0%">
    </div>
  </td>

  <td style="padding:40px;width:75%;vertical-align:middle">
      <papertitle>Uni<sup>2</sup>Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection</papertitle>
      <br>
      <strong>Yubin Wang*</strong>, Zhikang Zou*, Xiaoqing Ye, Xiao Tan, Errui Ding, Cairong Zhao
      <br>
      <em></em>Under NeurIPS 2025 peer review
      <br>
      <a href="./Publications/pdf/uni2det.pdf">[PDF]</a>
      <br>
      <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
      <div class="content">
        <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">We present Uni<sup>2</sup>Det, a brand new framework for unified and universal multi-
          dataset training on 3D detection, enabling robust performance across diverse
          domains and generalization to unseen domains. Due to substantial disparities in
          data distribution and variations in taxonomy across diverse domains, training such
          a detector by simply merging datasets poses a significant challenge. Motivated by
          this observation, we introduce multi-stage prompting modules for multi-dataset 3D
          detection, which leverages prompts based on the characteristics of corresponding
          datasets to mitigate existing differences. This elegant design facilitates seamless
          plug-and-play integration within various advanced 3D detection frameworks in
          a unified manner, while also allowing straightforward adaptation for universal
          applicability across datasets. Experiments are conducted across multiple dataset
          consolidation scenarios involving KITTI, Waymo, and nuScenes, demonstrating
          that our Uni2Det outperforms existing methods by a large margin in multi-dataset
          training. Furthermore, results on zero-shot cross-dataset transfer validate the
          generalization capability of our proposed method.
        </p>
        </h6>

      <script>
      function toggleContent(element) {
        var content = element.nextElementSibling;
        if (content.style.display === "none") {
          content.style.display = "block";
          element.textContent = "▼ Abstract";
        } else {
          content.style.display = "none";
          element.textContent = "▶ Abstract";
        }
      }
      </script>
      <div class="w3-content" style="text-align: justify">
      <p>
      </p>
      </div>
  </td>
  </script>
</body>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
  <td style="padding:10px;width:25%;height:80%;vertical-align:middle">
    <div class="one" >
        <img src='./Publications/img/hpt++.png' style="height:100%;width:160%; position: absolute;top: -0%">
    </div>
  </td>

  <td style="padding:40px;width:75%;vertical-align:middle">
      <papertitle>LLMs are Expert in Classifying Images: Hierarchically Prompting Vision-Language Models with Structured Linguistic Knowledge</papertitle>
      <br>
      <strong>Yubin Wang</strong>, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, Cairong Zhao
      <br>
      <em></em>Under IJCV peer review
      <br>
      <a href="./Publications/pdf/hpt++.pdf">[PDF]</a>
      <br>
      <div class="arrow" onclick="toggleContent(this)">▶ Abstract</div>
      <div class="content">
        <h6 style="font-family:calibri;font-weight:lighter;color:#3c3b34">Prompt learning has become a prevalent strategy for adapting vision-language foundation models
          (VLMs) such as CLIP to downstream tasks. With the emergence of large language models (LLMs),
          recent studies have explored the potential of using category-related descriptions to enhance prompt
          effectiveness. However, conventional descriptions lack explicit structured information necessary to
          represent the interconnections among key elements like entities or attributes with relation to a particular
          category. Since existing prompt tuning methods give littie consideration to managing structured
          knowledge, this paper advocates leveraging LLMs to construct a graph for each description to prioritize
          such structured knowledge. Consequently, we propose a novel approach called Hierarchical
          Prompt Tuning (HPT), enabling simultaneous modeling of both structured and conventional linguistic
          knowledge. Specifically, we introduce a relationship-guided attention module to capture pair-wise
          associations among entities and attributes for low-level prompt learning. In addition, by incorporating
          high-level and global-level prompts modeling overall semantics, the proposed hierarchical structure
          forges cross-level interlinks and empowers the model to handle more complex and long-term relationships.
          Finally, by enhancing multi-granularity corpus generation, redesigning the relationship-driven
          attention re-weighting module, and incorporating consistent constraints on the hierarchical text
          encoder, we propose HPT++, which further improves the performance of HPT. Our experiments are
          conducted across a wide range of evaluation settings, including base-to-novel generalization, cross-dataset
          evaluation, and domain generalization. Extensive results and ablation studies demonstrate
          the effectiveness of our methods, which consistently outperform existing SOTA methods.
        </p>
        </h6>

      <script>
      function toggleContent(element) {
        var content = element.nextElementSibling;
        if (content.style.display === "none") {
          content.style.display = "block";
          element.textContent = "▼ Abstract";
        } else {
          content.style.display = "none";
          element.textContent = "▶ Abstract";
        }
      }
      </script>
      <div class="w3-content" style="text-align: justify">
      <p>
      </p>
      </div>
  </td>
  </div>

        <!-- ------------------------ News------------------------------- -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>About Me</heading>
            <hr />
            <p>
              I'm Wang Yubin. My hometown is Fuzhou, Fujian Province, China. I am currently pursuing a master degree at 
              Tongji University in Shanghai, focusing on computer vision and multi-modal learning.<br><br>
              I am very interested in various sports, and in my spare time, I enjoy cycling, 
              playing badminton, playing football, watching sports games like NBA, F1, UEFA Champoinships League
              and so on. My favorite NBA star is Chris Paul, my favorite football team is Bayern Munich, and my favorite player is 
              Thomas Müller. I also like to draw inspiration from music, and some of my favorite artists include David Tao, 
              Jude Chiu, Stefanie Sun, LaLa Hsu, Shawn Mendes, Harry Styles, and Olivia Rodrigo. My MBTI personality is ISFJ.<br><br>
              
              I hope to meet more like-minded friends through this platform, so we can exchange ideas and grow together!
            </p>
          </td>
        </tr>
      </tbody></table>

</html>
